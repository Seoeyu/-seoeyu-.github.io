<!-- 

    Computation in Design
    Live Project March - April 2020.

    BA Design Communication, Faculty of Design.
    LASALLE College of the Arts.

    A Microsite template by Andreas Schlegel.
    Please make changes to the document below 
    and the linked css and image files.
    
    Add your own content and replace the 
    placeholder. Do read the comments.
-->

<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Proprioception</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

    <!-- 
            load all css files. Note that style sheets for different
            sections are placed into their own css file.
        -->
    <link rel="stylesheet" type="text/css" href="./css/normalize.css">
    <link rel="stylesheet" type="text/css" href="./css/landing.css">
    <link rel="stylesheet" type="text/css" href="./css/content.css">
    <link rel="stylesheet" type="text/css" href="./css/gallery.css">
    <link rel="stylesheet" type="text/css" href="./css/sticky.css">
    <link rel="stylesheet" type="text/css" href="./css/main.css">

    <!-- 
            load a set fonts fonts eg. from the google fonts repository 
        -->
    <link href="https://fonts.googleapis.com/css?family=Fira+Sans|Noto+Sans|Roboto|Roboto+Slab:100,300,400,700&display=swap" rel="stylesheet">
</head>




<!-- <style>
    
    body{
        margin: 0px;
        border: 0px;
        
    }
    
    #navi{
        width: 100%;
        height: 70px;
        background-color: black;
        color: white;
        position: fixed;
        top: 0;
   
         text-align:center;
    transform-origin: center left;
 
        right:0px;
    transform: translateX(90%) rotate(90deg);
        
    }
        #linkref{
            color: white;
            text-decoration: none;
            padding: 15px;
            text-align: center;
        }
        
    
    
    
    </style> -->

<body bgcolor="black">

    <!--   
<center>
<div id="navi">
    <a href="" id="linkref">artefact</a>
    <a href="" id="linkref">artefact</a>
    <a href="" id="linkref">artefact</a>
    <a href="" id="linkref">artefact</a>
    </div>
</center>    
    
    
<div id="data">
</div>
-->



    <div id="landing-video" class="landing landing-video">

        <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted">

            <source src="images/CiD_3_2-2.mp4" type="video/mp4">

        </video>
    </div>






    <div id="landing-image" class="landing landing-image">
        <section>
            <h2>Proprioception</h2>
            <h3>Computation in Design by Jack, Seoe and Toby</h3>
            <p>Proprioception is defined as the perception or awareness of the position and movement of the body, as well as the space around it. Also known as kinaesthesia, proprioceptive senses include balance, body position and spatial awareness. </p>
        </section>
    </div>




    <!-- 3
            the main section.
        -->
    <div id="main" class="main">

        <div id="summary" class="content">
            <h3>Summary</h3>
            <p> This project tackles two interrelated ideas: ‘computation in design’ and ‘critical making’. Starting with simple applications, we engaged in ‘play’ that involves technologies. By experimenting existing codes and recreating the codes. My group developed a series of ‘design probes’through technological experiments and applications. 
            </p>
        </div>



        
           
      

    </div>



    <div id="artefact" class="content">
        <h3>Artefact</h3>

        <h1>Primary Research & Observations</h1>


        <div id="artefact-images" class="content-image">
            <img src="images/23.jpg" class="n" />
            <img src="images/19.jpg" class="n" />
        </div>

        <h4>OBSERVATION 01 — JACK: Observations at the Gym</h4>

        <p>Observations of possible human body
            positions and ranges of movement through various
            exercises at the gym provided insight into feasible input
            mechanisms that involved physical motions of the
            human body or bodies.</p>


        <br>
        <br>
        <br>







        <div id="artefact-images" class="content-image">
            <img src="images/45.jpg" class="o" />

        </div>


        <h4>OBSERVATION 02 — SEOE: Interactions from Population</h4>

        <p>For my observations, I focused on interactions
            within the public space (outside Plaza
            Singapura, Dhoby Ghaut). I noticed trends
            between the
            interactions happening
            in the common area
            (e.g. walking, smoking).
            The more interactions with people around the area, seemingly more LED flashings
            would occur on the LED wall/screen of the building. This movement of population in the
            space gave insight into possible outputs of an LED nature.
            One possible study of relationships can be between the amount of human interactions
            in a space and the movement of the population within the space. Hence, more
            interaction (a floating population) = more movement of population in the space = more
            output data.</p>



        <br>
        <br>
        <br>



        <div id="artefact-images" class="content-image">
            <img src="images/44.jpg" class="o" />

        </div>


        <h4>OBSERVATION 03 — TOBY: Muscle and Spatial Memory</h4>

        <p>We often rely on our
            eyes to gauge and determine the proximity of a
            distance between us and an object. Our eyes
            function as a measuring tool that aids us in our
            mobility on a daily basis. For instance, we gauge
            with our eyes, how far to reach our arms to push
            a door. Likewise, we gauge with our eyes, how
            far to step to avoid tripping over a bump in the
            road. How reliant are we on our eyes?
            With repeated motions, it is possible to be able to gauge objects without using our eyes.
            Using the example of a keyboard, many are able to type sentences without looking
            down at the screen or at their keyboard. This is due to the daily routine of using their
            keyboard. Repeated motions result in muscle memory, that help us to navigate a
            familiar spatial space.
            How would humans perform in their ability to gauge spatial distance and proximity,
            without their sight?
            Would we able to approximate distances and quickly adapt to specific points in space
            without our eyes?</p>
        <!-- first block of images -->
        
        <br>
        
        
        <p>During our preliminary research phase, we obtained information regarding various types of proprioceptive input mechanisms. We achieved this by firstly, conducting on-site research and observations at locations that feature lots of movement per our choosing and secondly, analysing case studies comprising related interactive artworks done in the past that deploy the movement of human bodies as an input mechanism.</p>

         <p> Insights gained consist of input methodologies deploying spatial and muscle memory using repeated motions, existing installations in Singapore that deploy population-sized input, and possible body positions one may take while standing upright (and stationary): an arcing motion of the limbs, pushing a button, pulling a handle or lever, bending, squatting, stretching, twisting, turning and jumping. We also discovered the potential use of feet as sensors, or points of contact to the environment around (or beneath) us. </p>





    </div>


    <div id="artefact" class="content">


        <h1>Case Studies</h1>

        <h4>CASE STUDY 01 — Blooming Passage, Jewel</h4>

        <p>Velocity Induced Input (2 States)
            Permanent Installation for the Canopy Mazes at Singapore’s Jewel Changi
            Airport. “Blooming Passage” is permanently installed in the hedge maze of
            Canopy Park, located on the top-most floor of the complex. The colorful,
            motion-activated flowers react to passersby in the maze, offering a unique
            experience that changes with time.</p>
        <div>


            <p>Click <a href="https://www.youtube.com/watch?v=sJEB-cGSYfw" target="_blank">here</a> for the video of Blooming Passage.</p>
        </div>


        <br>





        <h4>CASE STUDY 02 — Artificial Intelligence, Yamaha</h4>

        <p>Velocity Induced Input (Variable States)</p>
        <p>
            Yamaha artificial intelligence (AI) technology enabled a world-renowned dancer
            Kaiji Moriyama to control a piano by his movements. The concert, held in Tokyo
            on November 22, 2017, was entitled "Mai Hi Ten Yu"' and was sponsored by
            Tokyo University of the Arts and Tokyo University of the Arts COI. Yamaha
            provided an original system, which can translate human movements into musical
            expression by using AI technology, as technical cooperation for the concert.</p>
        <div>


            <p>Click <a href="https://www.youtube.com/watch?v=mPwc02rdJfI" target="_blank">here</a> for the video of Artificial Intelligence.</p>
        </div>

        <br>



        <h4>CASE STUDY 03 — The Pool, Jen Lewins</h4>

        <p>Population-Sized Input</p>
        <p>
            The Pool is an interactive environment where movement creates swirling light
            and color. Users play on concentric rings of circular pads that communicate with
            each other wirelessly. As users shift their weight or move from one pad to
            another, their motions are reflected first on their pad, than broadcast to the other
            pads in the pool. As multiple users play in the pool, their interactions become
            mesmerizing patterns of shifting and fading colors. The Pool is composed of 106
            pads. Each pad is 3 feet in diameter and 6 inches tall. In plan, The Pool can span
            anywhere between a 37-foot square (at a compressed scale) to a 75-foot square
            (at an expanded scale).</p>
        <div>


            <p>Click <a href="https://jenlewinstudio.com/the-pool" target="_blank">here</a> for the video of The Pool.</p>
        </div>


        <br>



        <h4>CASE STUDY 04 — Hakanai, Adrien M & Claire B</h4>

        <p>Position-based Input</p>
        <p>
            Le mouvement de l'air, Adrien M & Claire B. Dance choreography performed in
            the immersive environment of a moving cube, to explore the fleeting nature of
            dreams and the fugacity of life.
            “In Japanese, "Hakanai" means ‘impermanent, fragile, evanescent, transitory,
            and fleeting’, somewhere between dream and reality. This symbolic association
            is the starting point for a dancer who will face images revealing what lies on the
            brink of imagination and reality.”</p>
        <!-- first block of images -->


        <div>


            <p>Click <a href="https://youtu.be/qAYAuJ9dIlE?t=23" target="_blank">here</a> for the video of Hakanai.</p>
        </div>


        <br>
        <br>
        <br>


        <h4>CONCLUSION from the case study</h4>
        <p> Two Different Velocity Types
            <br> 1 - 2 States
            <br>Yes Movement
            <br>No Movement

            <br>

            <br>2 - Variable States
            <br> Acceleration/ Speed of Movement
            <br> |--------------|
            <br>1 {scale} 100


        </p>
        
        
        
           <br>
        
            <p> The application of proprioception onto design through computational mediums presents an increased potential of interaction beyond audiovisual interfaces.
            </p>






    </div>



    <div id="artefact" class="content">


        <h1>Initial Sketches</h1>

        <div id="artefact-images" class="content-image">
            <img src="images/09.jpg" class="p" />
            <img src="images/05.jpg" class="p" />
            <img src="images/07.jpg" class="p" />


        </div>
        <br>

        <div id="artefact-images" class="content-image">
            <img src="images/42.jpg" class="m" />

        </div>

    </div>




    <div id="artefact" class="content">

        <h1>Case Studies 2</h1>

        <p>Case study based on secondary research of existing work that use physical motion and camera with different outputs.
        </p>




        <h4>Posnet</h4>

        <div id="artefact-images" class="content-image">
            <img src="images/43.jpg" class="n" />
            <img src="images/39.jpg" class="n" />
        </div>



        <p>PoseNet is used to capture the location of your hands within the video. The location then gets converted to music.</p>

        <br>



        <h4>musical sampler</h4>

        <div id="artefact-images" class="content-image">
            <img src="images/35.jpg" class="n" />
            <img src="images/37.jpg" class="n" />
        </div>




        <p>Musical sampler that can be triggered in different ways. Users can use their mouse and keyboard or tracking points on body by turing on the webcam.
        </p>


        <br>
        <Br>

        <h4>Execution from the case study</h4>
        <p> - Multi-positional Input through Posenet
            <br> - Multiple Invisible Shapes to Trigger Visuals and Audio
            <br> - Different instruments are played as well as different visuals

            <br>

            <br> Audio and visual output will be played when users hover specific body parts over the trigger areas.

        </p>


        </Br>
    </div>




    <div id="artefact" class="content">

        <h1>Experiments</h1>

        <h4>Individual experiments</h4>


        <h4></h4>

        <div id="artefact-images" class="content-image">
            <img src="images/33.jpg" class="n" />
            <img src="images/34.jpg" class="n" />
            <img src="images/CiD_3_18879_2-2_3.png" class="m" />



        </div>

        <br>

        <p>Using existing p5js, tried to Define the code how to trigger the audio or visual by touching the button and combining the two code.
        </p>


        <p>Visual output experiment on <a href="https://editor.p5js.org/SEOE/sketches/Qq2a5Cy1q
" target="_blank">here</a> and audio output experiment <a href="https://editor.p5js.org/SEOE/sketches/sM7eWhEZq
" target="_blank">here</a>.</p>

        <br>





        <Br>


        <h4>Developments</h4>

        <div id="landing-video" class="content-image video">

            <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted">

                <source src="images/CiD_video.mp4" type="video/mp4">

            </video>


        </div>




        <p>

            For the first draft of the project Proprioception, there were issues. Main issues are:
            <br>

            <br> 01. Awry pose detection, particularly
            in the wrist
            <br> 02. Unable to trigger sounds
            <br>03. Selection of sounds not suitable
            <br> 04. Visuals are not very engaging

            <br>
            <br>We convert our
            code into 2 separate sketches, one
            receiving and one broadcasting.
            This helped to solve the huge lag
            and the awry pose detection.
            P5js of broadcast and receiving for the Posenet need to be in the seperate window under the same screen.</p>

        <br>


        <div id="artefact-images" class="content-image">
            <img src="images/46.jpg" class="n" />
            <img src="images/18.jpg" class="n" />
        </div>

        <p> To experiment live p5js of the process of Prioception, click <a href="https://editor.p5js.org/SEOE/sketches/uRfGWRkP_
" target="_blank">Broadcast</a> and series of experiments in <a href="https://editor.p5js.org/SEOE/sketches/edIHncSXh
" target="_blank">Recieving 1</a>,
            <a href="https://editor.p5js.org/SEOE/sketches/_7IAoUPvj

" target="_blank">Recieving 2</a>,
            <a href="https://editor.p5js.org/SEOE/sketches/XFlkW3TEK

" target="_blank">Recieving 3</a>.
        </p>

        <br>

        <h4> Final p5js</h4>
        <p> We had two different p5js for the final.
            in black and white background different visual output with same audio output. Only or black version, we had background music. We decided to user testing for both.</p>
        <div id="artefact-images" class="content-image">
            <img src="images/black-1.png" class="n" />
            <img src="images/white-1.png" class="n" />
        </div>



        <p> Click <a href="https://editor.p5js.org/jackly/sketches/MF_mq-toO" target="_blank">here </a> for the black version and <a href="https://editor.p5js.org/toby/sketches/6lGgkwwWg" target="_blank">here</a> for the white version.


        </p>
        <p> Don't forget to turn on the <a href="https://editor.p5js.org/SEOE/sketches/uRfGWRkP_
" target="_blank">Broadcast</a> !


        </p>









        <br>


        <h4>Reflection</h4>

        <p> My group was too ambitious at the very first conceptualizing but managed to narrow down our idea. Our idea was the audience triggering the button as an input and sound and visual output together. I had done a lot of research on case studies to make the project more practical. The very first two p5js using PoseNet was our motivation to work on the sound output and visual output together. I tried combining two p5js examples which I found from the case study. Even though I had difficulties making the p5js to work, playing around with the p5js PoseNet was experimental and interesting. Rather than experimenting to make the p5js operate, I understood more easily when Andreas and my group mates gave me operable coding to look at. I was able to get feedback on my coding from their work and developments. Furthermore, I think it’s possible to make the Pump It Up game through the movements happening in square using PoseNet from p5js. Also, referring proprioception, detecting users walk flow in the public space when they pass the camera and make different sound output as they take each step. Working in groups for the artefact was great to get more deeper experimenting of p5js and experiencing existing p5js based on PoseNet. For future project, rather than understanding the existing coding that operates well, I would like to develop new project till end by myself next time.
        </p>



        </Br>

    </div>




    <div id="testing" class="content">



        <h3>Testing</h3>



        <!-- img tag for image(s) -->

        <p>We decided we wanted to conduct the user testing in public space. To evoke the feeling of play, community and togetherness. Before the actual user testing, we used oursevles as a test subject to experience the final two p5js in bigger screen. Result of p5js was good. Using big screen, projector as more fun and more engaging. </p>

        <br>

        <div id="landing-video" class="content-image video">

            <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted" >

                <source src="images/usertesting_1.mov" type="video/mp4">

            </video>


        </div>
        <br>

        <h1>2 Scenarios</h1>

        <p> We choose to test two different p5js sketch in black and white background. We had 2 scenarios, for Scenario A, the user will try the sketch alone. For the Scenario B, the user will try the skecth with another user. </p>
            
              <div id="artefact-images" class="content-image">
            <img src="images/CiD_3_3-2_user2-1.JPG" class="n" />
            <img src="images/IMG_4823.JPG" class="n" />
        </div>
        
        
        
        
        
        <p>
        User testing required projector, bigger screen, large wide floor area or a public space. Unfortunately, due to the COVID-19 situation, we conducted our user testing at home with three different users. </p>
        
        <br>
        <h4>User 01 — Leonard, Age 16 </h4>
        
           <div id="landing-video" class="content-image video">

            <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted" >

                <source src="images/CiD_3_3-2_user3-4(1).mp4" type="video/mp4">

            </video>


        </div>
        
        <h4>User 02 — Yujin, Age 23 </h4>
        
            <div id="landing-video" class="content-image video">

            <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted" >

                <source src="images/CiD_3_3-2_user2-1.mp4" type="video/mp4">

            </video>


        </div>
        
        
        <h4>User 03 — Shirley, Age 60 </h4> 
        
         <div id="landing-video" class="content-image video">

            <video id="video-bg-elem" preload="auto" autoplay="true" loop="loop" muted="muted" >

                <source src="images/usetesting_2.mp4" type="video/mp4">

            </video>


        </div>
        
        
         
        <br>
        


        <p> From the user testing, we found their preferences. Accordingly, we edited the sketch from the feedback. They preferred the larger trigger points using the white square. also, theys preferred the black background as it was easy to see the visual output. They found that white pulsating circles were better than squares as it was less messy and distracting. We did achieve our original aim of letting people “play” with music and visuals. All users said that they felt that it was a fun and dynamic experience. We also observed users trying to get out of their normal range of motion and challenging themselves to feel uncomfortable. Ultimately, we achieved our goal of enabling users to experience more physical movement, spatial awareness and proprioception.
        </p>

        <p>
        The general consensus lies in the larger trigger points (square shapes big enough to touch and align with one another in a grid-manner) in the black version being easier to use and maneuver toward. The larger size of the squares mean that the trigger points are bigger, and hence easier to go to. In the white version, the circles were much smaller and tougher to trigger.</p>

        <p>Although users chose the black version in terms of a friendlier user interface, they
            all chose the white version in terms of the animated visuals that played in the background as it was less distracting and the line of circles looked like a music visualiser.</p>

        <p>We did achieve our original aim of letting people “play” with music and visuals. All users said that they felt that it was a fun and dynamic experience. We also observed users trying to get out of their normal range of motion and challenging themselves to feel uncomfortable. Ultimately, we achieved our goal of enabling users to experience more physical movement, spatial awareness and proprioception.


        </p>


    

        <br>


      
    
 
    </div>
    
     <div id="user testing" class="content">
    <h4>Reflection</h4>
    <p>

It was too bad that we weren’t able to test our project where we chose at the first place because of COVID-19. It would have been better if we were able to test in school public spaces (e.g. frass, student lounge). I tried best as possible to give more space for the users in different environments using different devices. Generally, feedbacks suggested from users came out as I expected. They gave feedback about the camera not detecting their body perfectly. They were frustrated how the movement needed to be slow and precise to be detected. They preferred experimenting together with their friends. They seem to have more fun and playful when they are using their feet and walking and jumping around in the bigger space and screen. Extending the user testing in the future, I want users size to be in actual size and in the same eye-level on the screen as in the real life. I want the project to be screened on the bigger screen using the projector or TV in the more open spaces. Users communicating with the project by interacting from proprioception (physical motion), PoseNet is an interactive vision model that was able to estimate the pose of a user from camera by estimating their key body joints. The more inputs there are the more outputs of sounds and visual users can get, it was well developed interactive project as my group intended. 
    </p>


    </div>

    

    <br>


    <div id="p5js-sketch bg" class="content p5js-image">
        <section>

            <h3>Final p5js Sketches</h3>
            <ul>
                <li><a href="https://editor.p5js.org/jackly/sketches/ojIDaSEHhE 
" target="_blank">Proprioception Receiver</a> a grid of dancing colors.</li>
                <li><a href="https://editor.p5js.org/SEOE/sketches/uRfGWRkP_
" target="_blank">Proprioception Broadcast</a> dancing wings made from boxes.</li>
            </ul>


        </section>
    </div>




    <div id="main" class="main">


        <div id="images" class="gallery">
            <h3>Propriocetion final</h3>

            <!-- first block of images -->
            <div id="sketches" class="gallery-list">
                <div class="gallery-list-item">
                    <figure>
                        <img src="./images/CiD_3_2-2_1.jpg" />
                        <figcaption>
                            <p>Broadcast</p>
                        </figcaption>
                    </figure>
                </div>
            </div>

            <!-- second block of images -->
            <div id="outcomes" class="gallery-list">
                <div class="gallery-list-item">
                    <figure>
                        <img src="./images/CiD_3_2-2_2.jpg" />
                        <figcaption>
                            <p></p>
                        </figcaption>
                    </figure>
                </div>
                <div class="gallery-list-item">
                    <figure>
                        <img src="./images/CiD_3_2-2_3.jpg" />
                        
                    </figure>
                </div>
                <div class="gallery-list-item">
                    <figure>
                        <img src="./images/CiD_3_2-2_1%20copy.jpg" />
                        
                    </figure>
                </div>

            </div>
        </div>




    </div>



    
    <div id="final video" class="landing">
    <video width=100% controls>
    <source src="images/CiD_3_2-2.mp4" type="video/mp4"> 
                                               
        </video>
         </div>


   
<!--
    <div id="artefact-images" class="content-image">
        <img src="images/i-1.jpg" class="m" />
        <img src="images/i-1.jpg" class="m" />
    </div>


    <div id="testing-images" class="content-image">
        <img src="images/i-1.jpg" class="l" />
    </div>

-->


    <!-- 
                Add a series of images  that illustrate the process and 
                outcomes of your work.
            -->

    <!-- Some more project and Author(s) details -->
    <div class="footer">
        <p>Computation in Design by Seoe<br />
            together with Jack and Toby.<br />
            2020
        </p>
    </div>

    <!-- and finally, the titles that stick to the sides of the window -->
    <div id="sticky">
        <h2 id="live-project">Computation in Design</h2>
        <h2 id="project-title">Proprioception</h2>
    </div>


  
    </body>

</html>
